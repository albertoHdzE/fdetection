{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34420be4",
   "metadata": {},
   "source": [
    "# Classic approach\n",
    "\n",
    "* Synthetic data (Faker)\n",
    "* Defined number and types of models\n",
    "* (simple) EDA\n",
    "* Simple feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cfeeb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Fraud Detection AutoML Pipeline\n",
      "==================================================\n",
      "Generating synthetic fraud detection dataset...\n",
      "Dataset generated successfully!\n",
      "Total samples: 500\n",
      "Fraud cases: 50\n",
      "Normal cases: 450\n",
      "Fraud ratio: 10.00%\n",
      "Preprocessing data...\n",
      "Training set: 400 samples\n",
      "Test set: 100 samples\n",
      "Training fraud ratio: 10.00%\n",
      "Setting up models...\n",
      "Setup 6 models for evaluation\n",
      "Evaluating models...\n",
      "\n",
      "Evaluating RF_Balanced...\n",
      "CV ROC-AUC: 1.0000 (+/- 0.0000)\n",
      "Test ROC-AUC: 1.0000\n",
      "Test PR-AUC: 1.0000\n",
      "\n",
      "Evaluating GB_Balanced...\n",
      "CV ROC-AUC: 0.9738 (+/- 0.1049)\n",
      "Test ROC-AUC: 0.9944\n",
      "Test PR-AUC: 0.9545\n",
      "\n",
      "Evaluating LR_Balanced...\n",
      "CV ROC-AUC: 1.0000 (+/- 0.0000)\n",
      "Test ROC-AUC: 1.0000\n",
      "Test PR-AUC: 1.0000\n",
      "\n",
      "Evaluating SVM_Balanced...\n",
      "CV ROC-AUC: 1.0000 (+/- 0.0000)\n",
      "Test ROC-AUC: 1.0000\n",
      "Test PR-AUC: 1.0000\n",
      "\n",
      "Evaluating RF_SMOTE...\n",
      "CV ROC-AUC: 1.0000 (+/- 0.0000)\n",
      "Test ROC-AUC: 1.0000\n",
      "Test PR-AUC: 1.0000\n",
      "\n",
      "Evaluating GB_Undersample...\n",
      "CV ROC-AUC: 0.9361 (+/- 0.1200)\n",
      "Test ROC-AUC: 0.9889\n",
      "Test PR-AUC: 0.9167\n",
      "\n",
      "==================================================\n",
      "MODEL SELECTION RESULTS\n",
      "==================================================\n",
      "\n",
      "Model Performance Ranking (by PR-AUC):\n",
      "----------------------------------------\n",
      "1. RF_SMOTE        | PR-AUC: 1.0000 | ROC-AUC: 1.0000\n",
      "2. RF_Balanced     | PR-AUC: 1.0000 | ROC-AUC: 1.0000\n",
      "3. LR_Balanced     | PR-AUC: 1.0000 | ROC-AUC: 1.0000\n",
      "4. SVM_Balanced    | PR-AUC: 1.0000 | ROC-AUC: 1.0000\n",
      "5. GB_Balanced     | PR-AUC: 0.9545 | ROC-AUC: 0.9944\n",
      "6. GB_Undersample  | PR-AUC: 0.9167 | ROC-AUC: 0.9889\n",
      "\n",
      "ðŸ† BEST MODEL: RF_SMOTE\n",
      "   PR-AUC: 1.0000\n",
      "   ROC-AUC: 1.0000\n",
      "   CV ROC-AUC: 1.0000 Â± 0.0000\n",
      "\n",
      "==================================================\n",
      "DETAILED EVALUATION: RF_SMOTE\n",
      "==================================================\n",
      "\n",
      "Classification Report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        90\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "--------------------\n",
      "True Negatives:  90\n",
      "False Positives: 0\n",
      "False Negatives: 0\n",
      "True Positives:  10\n",
      "\n",
      "Business Impact Metrics:\n",
      "-------------------------\n",
      "Fraud Detection Rate (Recall): 100.00%\n",
      "Precision (True Fraud / All Flagged): 100.00%\n",
      "False Positive Rate: 0.00%\n",
      "\n",
      "Top 5 Most Important Features:\n",
      "-----------------------------------\n",
      "1. device_risk_score         0.3880\n",
      "2. location_risk_score       0.2533\n",
      "3. amount                    0.1280\n",
      "4. avg_amount_last_30_days   0.0739\n",
      "5. account_age_days          0.0680\n",
      "\n",
      "âœ… AutoML Pipeline Complete!\n",
      "Best Model: RF_SMOTE\n",
      "\n",
      "ðŸ’¾ Dataset saved as 'fraud_detection_dataset.csv'\n",
      "\n",
      "ðŸ”® Example: Making predictions on test set\n",
      "Actual: NORMAL | Predicted: NORMAL | Probability: 0.000\n",
      "Actual: NORMAL | Predicted: NORMAL | Probability: 0.010\n",
      "Actual: NORMAL | Predicted: NORMAL | Probability: 0.000\n",
      "Actual: NORMAL | Predicted: NORMAL | Probability: 0.000\n",
      "Actual: NORMAL | Predicted: NORMAL | Probability: 0.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FraudDetectionAutoML:\n",
    "    def __init__(self, n_samples=500, n_fraud=50, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the Fraud Detection AutoML system\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Total number of samples to generate\n",
    "            n_fraud: Number of fraud cases (positive class)\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_samples = n_samples\n",
    "        self.n_fraud = n_fraud\n",
    "        self.n_normal = n_samples - n_fraud\n",
    "        self.random_state = random_state\n",
    "        self.fake = Faker()\n",
    "        Faker.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def generate_fraud_data(self):\n",
    "        \"\"\"Generate synthetic fraud detection dataset using Faker\"\"\"\n",
    "        print(\"Generating synthetic fraud detection dataset...\")\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        # Generate normal transactions (0 = not fraud)\n",
    "        for i in range(self.n_normal):\n",
    "            record = {\n",
    "                'transaction_id': self.fake.uuid4(),\n",
    "                'amount': np.random.normal(100, 50),  # Normal spending pattern\n",
    "                'merchant_category': np.random.choice(['grocery', 'gas', 'restaurant', 'retail', 'online'], \n",
    "                                                    p=[0.3, 0.2, 0.2, 0.2, 0.1]),\n",
    "                'hour_of_day': np.random.choice(range(6, 23), p=self._get_normal_hour_prob()),\n",
    "                'day_of_week': np.random.randint(0, 7),\n",
    "                'age': np.random.randint(18, 80),\n",
    "                'account_age_days': np.random.randint(365, 3650),  # 1-10 years\n",
    "                'num_transactions_today': np.random.poisson(3),\n",
    "                'avg_amount_last_30_days': np.random.normal(95, 30),\n",
    "                'location_risk_score': np.random.beta(2, 8),  # Lower risk for normal\n",
    "                'device_risk_score': np.random.beta(2, 8),\n",
    "                'is_weekend': 0,\n",
    "                'is_fraud': 0\n",
    "            }\n",
    "            \n",
    "            # Set weekend flag\n",
    "            if record['day_of_week'] in [5, 6]:\n",
    "                record['is_weekend'] = 1\n",
    "                \n",
    "            data.append(record)\n",
    "        \n",
    "        # Generate fraud transactions (1 = fraud)\n",
    "        for i in range(self.n_fraud):\n",
    "            record = {\n",
    "                'transaction_id': self.fake.uuid4(),\n",
    "                'amount': np.random.choice([\n",
    "                    np.random.normal(500, 200),  # High amount fraud\n",
    "                    np.random.normal(50, 20),    # Small amount fraud\n",
    "                    np.random.normal(1000, 300)  # Very high amount fraud\n",
    "                ], p=[0.5, 0.3, 0.2]),\n",
    "                'merchant_category': np.random.choice(['online', 'atm', 'unknown', 'retail', 'gas'], \n",
    "                                                    p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
    "                'hour_of_day': np.random.choice(range(0, 24), p=self._get_fraud_hour_prob()),\n",
    "                'day_of_week': np.random.randint(0, 7),\n",
    "                'age': np.random.randint(18, 80),\n",
    "                'account_age_days': np.random.choice([\n",
    "                    np.random.randint(1, 90),     # New accounts (higher risk)\n",
    "                    np.random.randint(90, 3650)   # Older accounts\n",
    "                ], p=[0.7, 0.3]),\n",
    "                'num_transactions_today': np.random.choice([\n",
    "                    np.random.poisson(1),         # Few transactions\n",
    "                    np.random.poisson(10)         # Many transactions (velocity)\n",
    "                ], p=[0.6, 0.4]),\n",
    "                'avg_amount_last_30_days': np.random.normal(150, 100),\n",
    "                'location_risk_score': np.random.beta(5, 3),  # Higher risk for fraud\n",
    "                'device_risk_score': np.random.beta(6, 2),\n",
    "                'is_weekend': 0,\n",
    "                'is_fraud': 1\n",
    "            }\n",
    "            \n",
    "            # Set weekend flag\n",
    "            if record['day_of_week'] in [5, 6]:\n",
    "                record['is_weekend'] = 1\n",
    "                \n",
    "            data.append(record)\n",
    "        \n",
    "        # Convert to DataFrame and shuffle\n",
    "        self.data = pd.DataFrame(data)\n",
    "        self.data = self.data.sample(frac=1, random_state=self.random_state).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Dataset generated successfully!\")\n",
    "        print(f\"Total samples: {len(self.data)}\")\n",
    "        print(f\"Fraud cases: {self.data['is_fraud'].sum()}\")\n",
    "        print(f\"Normal cases: {len(self.data) - self.data['is_fraud'].sum()}\")\n",
    "        print(f\"Fraud ratio: {self.data['is_fraud'].mean():.2%}\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def _get_normal_hour_prob(self):\n",
    "        \"\"\"Get probability distribution for normal transaction hours\"\"\"\n",
    "        # Normal transactions more likely during business hours\n",
    "        probs = np.ones(17) * 0.05  # 6-22 hours\n",
    "        probs[2:10] = 0.08  # 8-16 hours (business hours)\n",
    "        probs[10:15] = 0.06  # 16-21 hours (evening)\n",
    "        return probs / probs.sum()\n",
    "    \n",
    "    def _get_fraud_hour_prob(self):\n",
    "        \"\"\"Get probability distribution for fraud transaction hours\"\"\"\n",
    "        # Fraud more likely during off-hours\n",
    "        probs = np.ones(24) * 0.03\n",
    "        probs[0:6] = 0.06   # Late night/early morning\n",
    "        probs[22:24] = 0.05  # Late evening\n",
    "        probs[8:17] = 0.02   # Business hours (less likely)\n",
    "        return probs / probs.sum()\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the data for machine learning\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        le = LabelEncoder()\n",
    "        self.data['merchant_category_encoded'] = le.fit_transform(self.data['merchant_category'])\n",
    "        \n",
    "        # Select features for modeling\n",
    "        feature_cols = [\n",
    "            'amount', 'merchant_category_encoded', 'hour_of_day', 'day_of_week',\n",
    "            'age', 'account_age_days', 'num_transactions_today', \n",
    "            'avg_amount_last_30_days', 'location_risk_score', 'device_risk_score',\n",
    "            'is_weekend'\n",
    "        ]\n",
    "        \n",
    "        X = self.data[feature_cols]\n",
    "        y = self.data['is_fraud']\n",
    "        \n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale the features\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        print(f\"Training set: {len(self.X_train)} samples\")\n",
    "        print(f\"Test set: {len(self.X_test)} samples\")\n",
    "        print(f\"Training fraud ratio: {self.y_train.mean():.2%}\")\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Setup different ML models with various approaches to handle imbalanced data\"\"\"\n",
    "        print(\"Setting up models...\")\n",
    "        \n",
    "        # Calculate class weights for imbalanced data\n",
    "        class_weights = compute_class_weight('balanced', \n",
    "                                           classes=np.unique(self.y_train), \n",
    "                                           y=self.y_train)\n",
    "        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "        \n",
    "        # Model 1: Random Forest with balanced class weights\n",
    "        self.models['RF_Balanced'] = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            class_weight='balanced',\n",
    "            random_state=self.random_state,\n",
    "            max_depth=10\n",
    "        )\n",
    "        \n",
    "        # Model 2: Gradient Boosting with balanced class weights\n",
    "        self.models['GB_Balanced'] = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            random_state=self.random_state,\n",
    "            max_depth=6\n",
    "        )\n",
    "        \n",
    "        # Model 3: Logistic Regression with balanced class weights\n",
    "        self.models['LR_Balanced'] = LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            random_state=self.random_state,\n",
    "            max_iter=1000\n",
    "        )\n",
    "        \n",
    "        # Model 4: SVM with balanced class weights\n",
    "        self.models['SVM_Balanced'] = SVC(\n",
    "            class_weight='balanced',\n",
    "            random_state=self.random_state,\n",
    "            probability=True,\n",
    "            kernel='rbf'\n",
    "        )\n",
    "        \n",
    "        # Model 5: Random Forest with SMOTE\n",
    "        self.models['RF_SMOTE'] = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=self.random_state)),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=100, random_state=self.random_state))\n",
    "        ])\n",
    "        \n",
    "        # Model 6: Gradient Boosting with undersampling\n",
    "        self.models['GB_Undersample'] = ImbPipeline([\n",
    "            ('undersample', RandomUnderSampler(random_state=self.random_state)),\n",
    "            ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=self.random_state))\n",
    "        ])\n",
    "        \n",
    "        print(f\"Setup {len(self.models)} models for evaluation\")\n",
    "    \n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluate all models using cross-validation and test set performance\"\"\"\n",
    "        print(\"Evaluating models...\")\n",
    "        \n",
    "        # Use stratified k-fold for cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            \n",
    "            # Determine which data to use (scaled or original)\n",
    "            if 'SVM' in name or 'LR' in name:\n",
    "                X_train_data = self.X_train_scaled\n",
    "                X_test_data = self.X_test_scaled\n",
    "            else:\n",
    "                X_train_data = self.X_train\n",
    "                X_test_data = self.X_test\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(model, X_train_data, self.y_train, \n",
    "                                      cv=cv, scoring='roc_auc')\n",
    "            \n",
    "            # Fit model and make predictions\n",
    "            model.fit(X_train_data, self.y_train)\n",
    "            y_pred = model.predict(X_test_data)\n",
    "            y_pred_proba = model.predict_proba(X_test_data)[:, 1]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            roc_auc = roc_auc_score(self.y_test, y_pred_proba)\n",
    "            precision, recall, _ = precision_recall_curve(self.y_test, y_pred_proba)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'cv_auc_mean': cv_scores.mean(),\n",
    "                'cv_auc_std': cv_scores.std(),\n",
    "                'test_roc_auc': roc_auc,\n",
    "                'test_pr_auc': pr_auc,\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_pred_proba,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "            print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
    "            print(f\"Test PR-AUC: {pr_auc:.4f}\")\n",
    "    \n",
    "    def select_best_model(self):\n",
    "        \"\"\"Select the best performing model based on PR-AUC (better for imbalanced data)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL SELECTION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Sort models by PR-AUC (Precision-Recall AUC is better for imbalanced datasets)\n",
    "        sorted_models = sorted(self.results.items(), \n",
    "                              key=lambda x: x[1]['test_pr_auc'], \n",
    "                              reverse=True)\n",
    "        \n",
    "        print(\"\\nModel Performance Ranking (by PR-AUC):\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, (name, results) in enumerate(sorted_models, 1):\n",
    "            print(f\"{i}. {name:15} | PR-AUC: {results['test_pr_auc']:.4f} | ROC-AUC: {results['test_roc_auc']:.4f}\")\n",
    "        \n",
    "        # Select best model\n",
    "        best_model_name = sorted_models[0][0]\n",
    "        best_model_results = sorted_models[0][1]\n",
    "        \n",
    "        print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
    "        print(f\"   PR-AUC: {best_model_results['test_pr_auc']:.4f}\")\n",
    "        print(f\"   ROC-AUC: {best_model_results['test_roc_auc']:.4f}\")\n",
    "        print(f\"   CV ROC-AUC: {best_model_results['cv_auc_mean']:.4f} Â± {best_model_results['cv_auc_std']:.4f}\")\n",
    "        \n",
    "        return best_model_name, best_model_results\n",
    "    \n",
    "    def detailed_evaluation(self, model_name, model_results):\n",
    "        \"\"\"Provide detailed evaluation of the best model\"\"\"\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(f\"DETAILED EVALUATION: {model_name}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        y_pred = model_results['predictions']\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(\"-\" * 20)\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(f\"True Negatives:  {cm[0, 0]}\")\n",
    "        print(f\"False Positives: {cm[0, 1]}\")\n",
    "        print(f\"False Negatives: {cm[1, 0]}\")\n",
    "        print(f\"True Positives:  {cm[1, 1]}\")\n",
    "        \n",
    "        # Business metrics\n",
    "        print(\"\\nBusiness Impact Metrics:\")\n",
    "        print(\"-\" * 25)\n",
    "        precision = cm[1, 1] / (cm[1, 1] + cm[0, 1]) if (cm[1, 1] + cm[0, 1]) > 0 else 0\n",
    "        recall = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if (cm[1, 1] + cm[1, 0]) > 0 else 0\n",
    "        \n",
    "        print(f\"Fraud Detection Rate (Recall): {recall:.2%}\")\n",
    "        print(f\"Precision (True Fraud / All Flagged): {precision:.2%}\")\n",
    "        print(f\"False Positive Rate: {cm[0, 1] / (cm[0, 0] + cm[0, 1]):.2%}\")\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        model = model_results['model']\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            print(\"\\nTop 5 Most Important Features:\")\n",
    "            print(\"-\" * 35)\n",
    "            feature_names = [\n",
    "                'amount', 'merchant_category', 'hour_of_day', 'day_of_week',\n",
    "                'age', 'account_age_days', 'num_transactions_today', \n",
    "                'avg_amount_last_30_days', 'location_risk_score', 'device_risk_score',\n",
    "                'is_weekend'\n",
    "            ]\n",
    "            \n",
    "            importances = model.feature_importances_\n",
    "            feature_importance = list(zip(feature_names, importances))\n",
    "            feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for i, (feature, importance) in enumerate(feature_importance[:5], 1):\n",
    "                print(f\"{i}. {feature:25} {importance:.4f}\")\n",
    "        elif hasattr(model, 'named_steps') and hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "            print(\"\\nTop 5 Most Important Features:\")\n",
    "            print(\"-\" * 35)\n",
    "            feature_names = [\n",
    "                'amount', 'merchant_category', 'hour_of_day', 'day_of_week',\n",
    "                'age', 'account_age_days', 'num_transactions_today', \n",
    "                'avg_amount_last_30_days', 'location_risk_score', 'device_risk_score',\n",
    "                'is_weekend'\n",
    "            ]\n",
    "            \n",
    "            importances = model.named_steps['classifier'].feature_importances_\n",
    "            feature_importance = list(zip(feature_names, importances))\n",
    "            feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for i, (feature, importance) in enumerate(feature_importance[:5], 1):\n",
    "                print(f\"{i}. {feature:25} {importance:.4f}\")\n",
    "    \n",
    "    def run_automl_pipeline(self):\n",
    "        \"\"\"Run the complete AutoML pipeline\"\"\"\n",
    "        print(\"ðŸš€ Starting Fraud Detection AutoML Pipeline\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Generate data\n",
    "        self.generate_fraud_data()\n",
    "        \n",
    "        # Preprocess data\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # Setup models\n",
    "        self.setup_models()\n",
    "        \n",
    "        # Evaluate models\n",
    "        self.evaluate_models()\n",
    "        \n",
    "        # Select best model\n",
    "        best_model_name, best_model_results = self.select_best_model()\n",
    "        \n",
    "        # Detailed evaluation\n",
    "        self.detailed_evaluation(best_model_name, best_model_results)\n",
    "        \n",
    "        print(f\"\\nâœ… AutoML Pipeline Complete!\")\n",
    "        print(f\"Best Model: {best_model_name}\")\n",
    "        \n",
    "        return best_model_name, best_model_results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run the AutoML system\n",
    "    fraud_automl = FraudDetectionAutoML(n_samples=500, n_fraud=50, random_state=42)\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    best_model_name, best_results = fraud_automl.run_automl_pipeline()\n",
    "    \n",
    "    # Save the dataset for inspection\n",
    "    fraud_automl.data.to_csv('fraud_detection_dataset.csv', index=False)\n",
    "    print(f\"\\nðŸ’¾ Dataset saved as 'fraud_detection_dataset.csv'\")\n",
    "    \n",
    "    # Example of making predictions on new data\n",
    "    print(f\"\\nðŸ”® Example: Making predictions on test set\")\n",
    "    sample_indices = np.random.choice(len(fraud_automl.X_test), 5, replace=False)\n",
    "    for idx in sample_indices:\n",
    "        actual = fraud_automl.y_test.iloc[idx]\n",
    "        prob = best_results['probabilities'][idx]\n",
    "        prediction = \"FRAUD\" if prob > 0.5 else \"NORMAL\"\n",
    "        actual_label = \"FRAUD\" if actual == 1 else \"NORMAL\"\n",
    "        print(f\"Actual: {actual_label:6} | Predicted: {prediction:6} | Probability: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74576d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
